# Homa AI Package - Environment Variables Example
# Copy this file to .env and fill in your actual API keys

# =============================================================================
# DEFAULT PROVIDER CONFIGURATION
# =============================================================================
# Choose your default AI provider (openai, anthropic, grok, groq, gemini)
HOMA_PROVIDER=openai

# Default system prompt for all AI interactions
HOMA_SYSTEM_PROMPT="You are a helpful AI assistant."

# =============================================================================
# OPENAI CONFIGURATION
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_API_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=1000
OPENAI_TIMEOUT=30

# =============================================================================
# ANTHROPIC CONFIGURATION
# =============================================================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
ANTHROPIC_API_URL=https://api.anthropic.com/v1
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_MAX_TOKENS=1000
ANTHROPIC_TIMEOUT=30

# =============================================================================
# GROK CONFIGURATION
# =============================================================================
# Get your API key from: https://console.x.ai/
GROK_API_KEY=xai-your-grok-api-key-here
GROK_MODEL=grok-2
GROK_TEMPERATURE=0.7
GROK_MAX_TOKENS=1000

# =============================================================================
# GROQ CONFIGURATION (Ultra-fast inference)
# =============================================================================
# Get your API key from: https://console.groq.com/
GROQ_API_KEY=gsk_your-groq-api-key-here
GROQ_API_URL=https://api.groq.com/openai/v1
GROQ_MODEL=openai/gpt-oss-20b
GROQ_TEMPERATURE=0.7
GROQ_MAX_TOKENS=1000
GROQ_TIMEOUT=30

# =============================================================================
# GEMINI CONFIGURATION (Google AI)
# =============================================================================
# Get your API key from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_BASE_URI=https://generativelanguage.googleapis.com/v1beta
GEMINI_MODEL=gemini-2.0-flash-exp
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_TOKENS=1000
GEMINI_TIMEOUT=30

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Enable/disable logging of AI interactions
HOMA_LOGGING=false
HOMA_LOG_CHANNEL=stack

# =============================================================================
# CACHING CONFIGURATION
# =============================================================================
# Enable/disable response caching
HOMA_CACHE_ENABLED=false
HOMA_CACHE_TTL=3600
HOMA_CACHE_PREFIX=homa_

# =============================================================================
# AVAILABLE MODELS BY PROVIDER
# =============================================================================
# OpenAI Models:
# - gpt-5 (latest, most advanced)
# - gpt-5o (optimized GPT-5)
# - gpt-4o (latest GPT-4 with vision)
# - gpt-4o-mini (smaller, faster GPT-4o)
# - gpt-4-turbo (fast GPT-4 variant)
# - gpt-4 (most capable)
# - gpt-3.5-turbo (fast and cost-effective)# - gpt-3.5-turbo (fast and cost-effective)

# Anthropic Models:
# - claude-3-5-sonnet-20241022 (latest, most capable)
# - claude-3-opus-20240229 (most powerful)
# - claude-3-sonnet-20240229 (balanced)
# - claude-3-haiku-20240307 (fastest, most cost-effective)

# Grok Models:
# - grok-2 (default)
# - grok-2-latest
# - grok-2-1212
# - grok-2-vision
# - grok-beta

# Groq Models (Ultra-fast inference):
# - openai/gpt-oss-20b (large, capable)
# - openai/gpt-oss-7b (smaller, faster)
# - llama-3.1-70b-versatile (Meta Llama)
# - llama-3.1-8b-instant (fast Llama)
# - mixtral-8x7b-32768 (Mixtral)
# - gemma-7b-it (Google Gemma)


# Gemini Models (Google AI with multimodal capabilities):
# - gemini-2.0-flash-exp (latest, fastest)
# - gemini-1.5-pro-latest (most capable)
# - gemini-1.5-flash-latest (balanced speed and capability)
# - gemini-1.5-pro (stable pro model)
# - gemini-1.5-flash (fast and efficient)
# - gemini-1.5-pro-002 (versioned pro model)
# - gemini-1.5-flash-002 (versioned flash model)
# =============================================================================
# USAGE EXAMPLES
# =============================================================================
# Basic usage:
# $response = Homa::ask('What is Laravel?');

# With specific provider:
# $response = Homa::provider('groq')->ask('Hello!');

# With configuration:
# $response = Homa::model('gpt-4')->temperature(0.7)->ask('Question');

# Conversations:
# $conversation = Homa::startConversation();
# $response = $conversation->ask('Hello!');
# $response = $conversation->ask('What did I just say?');
